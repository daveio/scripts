#!/usr/bin/env -S bun --enable-source-maps
import { exec } from "node:child_process";
import * as fs from "node:fs";
import * as path from "node:path";
import { promisify } from "node:util";
import { Command } from "commander";
import * as yaml from "js-yaml";

const execAsync = promisify(exec);

// Function to generate timestamp string in YYYY-MM-DD_HH-MM-SS format
function getTimestamp(): string {
	const now = new Date();
	const year = now.getFullYear();
	const month = String(now.getMonth() + 1).padStart(2, "0");
	const day = String(now.getDate()).padStart(2, "0");
	const hours = String(now.getHours()).padStart(2, "0");
	const minutes = String(now.getMinutes()).padStart(2, "0");
	const seconds = String(now.getSeconds()).padStart(2, "0");
	return `${year}-${month}-${day}_${hours}-${minutes}-${seconds}`;
}
const _MAX_CONCURRENCY = 5;

// Type definitions
interface ActionUpdate {
	actionPath: string;
	oldRef: string;
	newRef: string;
}

interface WorkflowUpdate {
	filePath: string;
	relativePath: string;
	updates: ActionUpdate[];
}

interface RepoUpdate {
	repoName: string;
	workflowUpdates: WorkflowUpdate[];
}

interface ProcessingSummary {
	repoUpdates: RepoUpdate[];
	errors: string[];
	totalReposProcessed: number;
	totalFilesProcessed: number;
	totalActionsUpdated: number;
}

interface RepoMetadata {
	defaultBranch: string;
	latestCommit: string;
}

interface GitHubRepoCache {
	[repoKey: string]: RepoMetadata;
}

// Find all repositories in a directory
function findRepositories(rootDir: string): string[] {
	const repos: string[] = [];

	try {
		const entries = fs.readdirSync(rootDir, { withFileTypes: true });

		for (const entry of entries) {
			if (entry.isDirectory() && !entry.name.startsWith(".")) {
				const repoPath = path.join(rootDir, entry.name);

				// Check if it's a git repository
				if (fs.existsSync(path.join(repoPath, ".git"))) {
					repos.push(repoPath);
				}
			}
		}
	} catch (error) {
		console.error(
			`Error reading directory ${rootDir}: ${error instanceof Error ? error.message : String(error)}`,
		);
	}

	return repos;
}

// Extract all unique GitHub repositories from workflow files
function extractUniqueRepos(repositoriesToProcess: string[]): Set<string> {
	const uniqueRepos = new Set<string>();

	for (const repoPath of repositoriesToProcess) {
		const workflowsDir = path.join(repoPath, ".github", "workflows");
		if (!fs.existsSync(workflowsDir)) continue;

		const workflowFiles: string[] = [];
		const dirEntries = fs.readdirSync(workflowsDir, { withFileTypes: true });

		for (const entry of dirEntries) {
			if (
				entry.isFile() &&
				(entry.name.endsWith(".yml") || entry.name.endsWith(".yaml"))
			) {
				workflowFiles.push(path.join(workflowsDir, entry.name));
			}
		}

		for (const filePath of workflowFiles) {
			try {
				const content = fs.readFileSync(filePath, "utf-8");
				const workflowYaml = yaml.load(content) as Record<string, unknown>;

				// Extract action references from the YAML
				const extractReposFromNode = (
					node: Record<string, unknown> | unknown[] | unknown,
				): void => {
					if (!node || typeof node !== "object") return;

					if (Array.isArray(node)) {
						for (const item of node) {
							extractReposFromNode(item);
						}
						return;
					}

					for (const key in node as Record<string, unknown>) {
						const typedNode = node as Record<string, unknown>;
						if (key === "uses" && typeof typedNode[key] === "string") {
							const actionRef = (typedNode[key] as string).trim();

							// Skip Docker references and local references
							if (
								actionRef.startsWith("./") ||
								actionRef.startsWith("docker://")
							)
								continue;

							// Check if reference contains a version/commit
							if (actionRef.includes("@")) {
								const splitResult = actionRef.split("@");
								if (
									splitResult.length < 2 ||
									!splitResult[0] ||
									!splitResult[1]
								)
									continue;

								const actionPath = splitResult[0];
								const repoPathParts = actionPath.split("/");
								if (repoPathParts.length < 2) continue;

								const owner = repoPathParts[0];
								const repo = repoPathParts[1];
								const fullRepo = `${owner}/${repo}`;
								uniqueRepos.add(fullRepo);
							}
						} else {
							extractReposFromNode(typedNode[key]);
						}
					}
				};

				extractReposFromNode(workflowYaml);
			} catch (error) {
				console.error(`Error reading workflow file ${filePath}: ${error}`);
			}
		}
	}

	return uniqueRepos;
}

// Build cache of repository metadata using GitHub GraphQL API
async function buildRepoCache(
	uniqueRepos: Set<string>,
): Promise<GitHubRepoCache> {
	const cache: GitHubRepoCache = {};

	console.log(
		`üîç Fetching metadata for ${uniqueRepos.size} unique repositories using GraphQL...`,
	);

	// Get GitHub token
	const { stdout: token } = await execAsync("gh auth token");
	const githubToken = token.trim();

	// Convert set to array and split into batches (GraphQL has query size limits)
	const repoArray = Array.from(uniqueRepos);
	const batchSize = 50; // Conservative batch size to avoid query limits
	const batches = [];

	for (let i = 0; i < repoArray.length; i += batchSize) {
		batches.push(repoArray.slice(i, i + batchSize));
	}

	console.log(`  üì¶ Processing ${batches.length} batch(es) of repositories...`);

	for (let batchIndex = 0; batchIndex < batches.length; batchIndex++) {
		const batch = batches[batchIndex];
		console.log(
			`  üîÑ Processing batch ${batchIndex + 1}/${batches.length} (${batch.length} repositories)...`,
		);

		try {
			// Build GraphQL query for this batch
			const repositoryQueries = batch
				.map((fullRepo, index) => {
					const [owner, name] = fullRepo.split("/");
					return `
          repo${index}: repository(owner: "${owner}", name: "${name}") {
            nameWithOwner
            defaultBranchRef {
              name
              target {
                ... on Commit {
                  oid
                }
              }
            }
          }`;
				})
				.join("\n");

			const query = `
        query {
          ${repositoryQueries}
        }`;

			// Execute GraphQL query
			const graphqlPayload = JSON.stringify({ query });
			const { stdout } = await execAsync(
				`curl -s -H "Authorization: bearer ${githubToken}" -H "Content-Type: application/json" -X POST -d '${graphqlPayload.replace(/'/g, "'\\''")}' https://api.github.com/graphql`,
			);

			const response = JSON.parse(stdout);

			if (response.errors) {
				console.error(`  ‚ùå GraphQL errors in batch ${batchIndex + 1}:`);
				console.error(response.errors);
				continue;
			}

			// Process the response data
			const data = response.data;
			let successCount = 0;

			for (let i = 0; i < batch.length; i++) {
				const fullRepo = batch[i];
				const repoData = data[`repo${i}`];

				if (repoData?.defaultBranchRef?.target) {
					const defaultBranch = repoData.defaultBranchRef.name;
					const latestCommit = repoData.defaultBranchRef.target.oid;

					cache[fullRepo] = {
						defaultBranch,
						latestCommit,
					};

					console.log(
						`    ‚úì Cached ${fullRepo}: ${defaultBranch}@${latestCommit.substring(0, 8)}...`,
					);
					successCount++;
				} else {
					console.log(
						`    ‚ùå Failed to get data for ${fullRepo} (repository may not exist or be accessible)`,
					);
				}
			}

			console.log(
				`  üìä Batch ${batchIndex + 1} completed: ${successCount}/${batch.length} repositories cached`,
			);
		} catch (error) {
			console.error(`  ‚ùå Failed to process batch ${batchIndex + 1}: ${error}`);

			// Fallback to individual REST API calls for this batch
			console.log(
				`  üîÑ Falling back to individual REST calls for batch ${batchIndex + 1}...`,
			);
			for (const fullRepo of batch) {
				try {
					const { stdout: branchStdout } = await execAsync(
						`gh repo view ${fullRepo} --json defaultBranchRef -q .defaultBranchRef.name`,
					);
					const defaultBranch = branchStdout.trim();

					const { stdout: commitStdout } = await execAsync(
						`gh api repos/${fullRepo}/commits/${defaultBranch} --jq .sha`,
					);
					const latestCommit = commitStdout.trim();

					cache[fullRepo] = {
						defaultBranch,
						latestCommit,
					};

					console.log(
						`    ‚úì Cached ${fullRepo}: ${defaultBranch}@${latestCommit.substring(0, 8)}... (REST fallback)`,
					);
				} catch (restError) {
					console.error(
						`    ‚ùå Failed to fetch ${fullRepo} via REST: ${restError}`,
					);
				}
			}
		}
	}

	console.log(
		`üì¶ Built cache for ${Object.keys(cache).length}/${uniqueRepos.size} repositories\n`,
	);

	return cache;
}

// Main function to coordinate the entire process
async function main() {
	console.log(
		"üîç Finding repositories and updating GitHub Action workflows...",
	);
	// Set up command-line interface using Commander
	const program = new Command();
	program
		.name("pin")
		.description(
			"Find repositories and update GitHub Action workflows to specific commit SHAs",
		)
		.argument("[directory]", "specific repository directory to process")
		.parse();

	// Determine what to process
	let repositoriesToProcess: string[] = [];

	if (program.args[0]) {
		// Process specific directory
		const targetPath = path.resolve(program.args[0]);

		if (!fs.existsSync(targetPath)) {
			console.error(`‚ùå Directory not found: ${targetPath}`);
			process.exit(1);
		}

		// Check if it's a repository itself
		if (fs.existsSync(path.join(targetPath, ".git"))) {
			repositoriesToProcess = [targetPath];
		} else {
			console.error(`‚ùå Not a git repository: ${targetPath}`);
			process.exit(1);
		}
	} else {
		// Process all subdirectories of the default path
		const defaultPath = "/Users/dave/src/github.com/daveio";
		console.log(`üîç Scanning for repositories in: ${defaultPath}`);
		repositoriesToProcess = findRepositories(defaultPath);
	}

	if (repositoriesToProcess.length === 0) {
		console.log("No repositories found to process");
		return;
	}

	console.log(
		`Found ${repositoriesToProcess.length} repository(ies) to process\n`,
	);

	// Extract unique repositories from all workflow files and build cache
	const uniqueRepos = extractUniqueRepos(repositoriesToProcess);
	const repoCache = await buildRepoCache(uniqueRepos);

	const homeDir = process.env.HOME || "~";
	const backupRoot = path.join(homeDir, ".actions-backups");
	// Ensure backup root directory exists
	if (!fs.existsSync(backupRoot)) {
		fs.mkdirSync(backupRoot, { recursive: true });
	}

	// Create timestamped directory for this run
	const timestamp = getTimestamp();
	const backupDir = path.join(backupRoot, timestamp);
	fs.mkdirSync(backupDir, { recursive: true });

	console.log(`üïí Creating backups in timestamped directory: ${backupDir}`);

	const summary: ProcessingSummary = {
		repoUpdates: [],
		errors: [],
		totalReposProcessed: 0,
		totalFilesProcessed: 0,
		totalActionsUpdated: 0,
	};

	// Process each repository
	for (const repoPath of repositoriesToProcess) {
		const repoName = path.basename(repoPath);
		try {
			const repoUpdate = await processRepository(
				repoName,
				repoPath,
				backupDir,
				summary,
				repoCache,
			);
			if (repoUpdate.workflowUpdates.length > 0) {
				summary.repoUpdates.push(repoUpdate);
			}
			summary.totalReposProcessed++;
		} catch (error) {
			summary.errors.push(`Error processing repository ${repoName}: ${error}`);
			console.error(
				`‚ùå Error processing repository ${repoName}: ${error instanceof Error ? error.message : String(error)}`,
			);
		}
	}

	// Print summary
	printSummary(summary, backupDir);
}

// Process a single repository
async function processRepository(
	repoName: string,
	repoPath: string,
	backupDir: string,
	summary: ProcessingSummary,
	repoCache: GitHubRepoCache,
): Promise<RepoUpdate> {
	console.log(`\nüìÅ Processing repository: ${repoName}`);
	console.log(
		`  üîÑ Backups will be stored in: ${path.join(backupDir, repoName)}`,
	);

	const repoUpdate: RepoUpdate = {
		repoName,
		workflowUpdates: [],
	};

	// Find workflow files
	const workflowsDir = path.join(repoPath, ".github", "workflows");
	if (!fs.existsSync(workflowsDir)) {
		console.log(`  No workflows directory found in ${repoName}`);
		return repoUpdate;
	}

	const workflowFiles: string[] = [];
	const dirEntries = fs.readdirSync(workflowsDir, { withFileTypes: true });

	for (const entry of dirEntries) {
		if (
			entry.isFile() &&
			(entry.name.endsWith(".yml") || entry.name.endsWith(".yaml"))
		) {
			workflowFiles.push(path.join(workflowsDir, entry.name));
		}
	}

	if (workflowFiles.length === 0) {
		console.log(`  No workflow files found in ${repoName}`);
		return repoUpdate;
	}

	console.log(`  Found ${workflowFiles.length} workflow files`);

	// Create backup directory for this repository
	const repoBackupDir = path.join(backupDir, repoName, ".github", "workflows");
	fs.mkdirSync(repoBackupDir, { recursive: true });

	// Process each workflow file
	const filePromises = workflowFiles.map((filePath) =>
		processWorkflowFile(filePath, repoPath, backupDir, repoCache),
	);

	const workflowUpdates = await Promise.all(filePromises);
	const validUpdates = workflowUpdates.filter(
		(update) => update !== null,
	) as WorkflowUpdate[];

	repoUpdate.workflowUpdates = validUpdates;
	summary.totalFilesProcessed += workflowFiles.length;
	summary.totalActionsUpdated += validUpdates.reduce(
		(total, update) => total + update.updates.length,
		0,
	);

	return repoUpdate;
}

// Process a single workflow file
async function processWorkflowFile(
	filePath: string,
	repoPath: string,
	backupDir: string,
	repoCache: GitHubRepoCache,
): Promise<WorkflowUpdate | null> {
	const relativePath = path.relative(repoPath, filePath);
	const backupPath = path.join(
		backupDir,
		path.basename(repoPath),
		relativePath,
	);

	try {
		// Create backup of the file
		const backupDirPath = path.dirname(backupPath);
		fs.mkdirSync(backupDirPath, { recursive: true });
		fs.copyFileSync(filePath, backupPath);

		// Read the file and parse YAML
		const content = fs.readFileSync(filePath, "utf-8");
		const workflowYaml = yaml.load(content) as Record<string, unknown>;

		// Find and process action references
		const updates: ActionUpdate[] = [];

		// Function to recursively update GitHub Actions in the YAML structure
		const processNode = (
			node: Record<string, unknown> | unknown[] | unknown,
		): boolean => {
			if (!node || typeof node !== "object") {
				return false;
			}

			let modified = false;
			// If this is an array, process each item
			if (Array.isArray(node)) {
				for (let i = 0; i < node.length; i++) {
					const childModified = processNode(node[i]);
					modified = modified || childModified;
				}
				return modified;
			}
			// Process object properties
			for (const key in node as Record<string, unknown>) {
				// If the key is 'uses', this might be a GitHub Action reference
				const typedNode = node as Record<string, unknown>;
				if (key === "uses" && typeof typedNode[key] === "string") {
					const actionRef = (typedNode[key] as string).trim();

					// Skip Docker references and local references
					if (actionRef.startsWith("./") || actionRef.startsWith("docker://")) {
						continue;
					}
					// Check if reference contains a version/commit
					if (actionRef.includes("@")) {
						const splitResult = actionRef.split("@");
						if (splitResult.length < 2 || !splitResult[0] || !splitResult[1]) {
							continue;
						}
						const actionPath = splitResult[0];
						const oldRef = splitResult[1];

						// Extract repo from action path (could be owner/repo or owner/repo/path)
						const repoPathParts = actionPath.split("/");
						if (repoPathParts.length < 2) {
							continue; // Invalid reference
						}
						const owner = repoPathParts[0];
						const repo = repoPathParts[1];
						const fullRepo = `${owner}/${repo}`;

						// Use cached data instead of making API calls
						const cachedRepo = repoCache[fullRepo];
						if (cachedRepo) {
							// Update the action reference in the YAML
							const latestCommit = cachedRepo.latestCommit;
							typedNode[key] = `${actionPath}@${latestCommit}`;
							updates.push({
								actionPath,
								oldRef,
								newRef: latestCommit,
							});
							console.log(
								`  üìå Pinned ${actionPath} from ${oldRef} to ${latestCommit.substring(0, 8)}...`,
							);
							modified = true;
						} else {
							console.log(
								`  ‚ö†Ô∏è  No cached data for ${fullRepo}, skipping ${actionPath}@${oldRef}`,
							);
						}
					}
				} else {
					// Recursively process nested objects and arrays
					const childModified = processNode(typedNode[key]);
					modified = modified || childModified;
				}
			}
			return modified;
		};
		// Process the entire YAML structure
		const modified = processNode(workflowYaml);
		// Write updated content if there were changes
		if (updates.length > 0 && modified) {
			// Convert back to YAML and write to file
			const updatedContent = yaml.dump(workflowYaml, {
				lineWidth: -1, // Prevent line wrapping
				noRefs: true, // Don't use reference tags for duplicate objects
				quotingType: '"', // Use double quotes for strings
			});
			fs.writeFileSync(filePath, updatedContent);
			console.log(
				`  ‚úÖ Updated ${relativePath} with ${updates.length} action references`,
			);

			return {
				filePath,
				relativePath,
				updates,
			};
		}

		return null;
	} catch (error) {
		console.error(
			`  ‚ùå Error processing ${relativePath}: ${error instanceof Error ? error.message : String(error)}`,
		);
		return null;
	}
}

// Print summary of all changes
function printSummary(summary: ProcessingSummary, backupDir: string) {
	console.log("\nüìä Summary of GitHub Action Updates");
	console.log("===============================");
	console.log(`Repositories processed: ${summary.totalReposProcessed}`);
	console.log(`Workflow files processed: ${summary.totalFilesProcessed}`);
	console.log(`Action references updated: ${summary.totalActionsUpdated}`);
	console.log(`Backup location: ${backupDir}`);

	if (summary.repoUpdates.length > 0) {
		console.log("\nüîÑ Changes made:");

		for (const repoUpdate of summary.repoUpdates) {
			console.log(`\nüìÅ Repository: ${repoUpdate.repoName}`);

			for (const workflowUpdate of repoUpdate.workflowUpdates) {
				console.log(`  üìÑ ${workflowUpdate.relativePath}`);

				for (const actionUpdate of workflowUpdate.updates) {
					console.log(`    - ${actionUpdate.actionPath}`);
					console.log(`      From: ${actionUpdate.oldRef}`);
					console.log(`      To:   ${actionUpdate.newRef}`);
				}
			}
		}
	} else {
		console.log("\nüìù No changes were made.");
	}

	if (summary.errors.length > 0) {
		console.log("\n‚ùå Errors:");
		for (const error of summary.errors) {
			console.log(`  - ${error}`);
		}
	}
}

// Run the main function
main().catch((error) => {
	console.error(
		`‚ùå Fatal error: ${error instanceof Error ? error.message : String(error)}`,
	);
	process.exit(1);
});
